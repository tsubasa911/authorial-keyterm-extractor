import streamlit as st
from modules.sidebar_lang import set_language

lang_code, lang_dict = set_language()

st.title(lang_dict[lang_code]["title_intro"])
st.caption(lang_dict[lang_code]["caption_intro"])

# Heroï¼ˆæ¦‚è¦ï¼‰
if lang_code == "ja":
    st.info(
        "ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ **è‹±èªã®å­¦è¡“ãƒ†ã‚­ã‚¹ãƒˆ** ã‹ã‚‰ *ã‚­ãƒ¼ã‚¿ãƒ¼ãƒ * ã‚’æŠ½å‡ºã—ã€"
        " **ãƒªã‚¹ãƒˆ / ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆ / ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰** ã§å¯è¦–åŒ–ã—ã¾ã™ã€‚"
        " åˆã‚ã¦ã§ã‚‚ã€ä¸‹ã®ã€Œã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã€ã§æœ€çŸ­ãƒ«ãƒ¼ãƒˆã‚’ç¢ºèªã§ãã¾ã™ã€‚"
    )
else:
    st.info(
        "This tool extracts *key terms* from **English academic texts** and "
        "visualizes them as **List / Bubble Chart / Word Cloud**. "
        "Check 'Quick Start' below for the fastest way to get started."
    )

st.divider()

# ä¸»ãªæ©Ÿèƒ½
st.subheader("âœ¨ " + ("Main Features" if lang_code == "en" else "ä¸»ãªæ©Ÿèƒ½"))
col1, col2, col3 = st.columns(3, gap="large")

with col1:
    st.markdown("### ğŸ“¥ " + ("Input" if lang_code == "en" else "å…¥åŠ›"))
    st.markdown(
        "- `.txt` upload or paste\n- English text only\n- Click **Start Analysis**"
        if lang_code == "en" else
        "- `.txt` ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ or ç›´æ¥å…¥åŠ›\n- è‹±èªãƒ†ã‚­ã‚¹ãƒˆå°‚ç”¨\n- **åˆ†æé–‹å§‹** ãƒœã‚¿ãƒ³ã§å®Ÿè¡Œ"
    )

with col2:
    st.markdown("### ğŸ§  " + ("Analysis" if lang_code == "en" else "åˆ†æ"))
    st.markdown(
        "- spaCy preprocessing (lemmatization/POS)\n- Remove stopwords/numbers/symbols\n- TF-IDF with reference corpus"
        if lang_code == "en" else
        "- spaCy å‰å‡¦ç†ï¼ˆlemmatize / POSï¼‰\n- ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰/æ•°å­—/è¨˜å·ã®é™¤å»\n- æ¯”è¼ƒã‚³ãƒ¼ãƒ‘ã‚¹ Ã— **TF-IDF** ã§ç‰¹å¾´èªæŠ½å‡º"
    )

with col3:
    st.markdown("### ğŸ“Š " + ("Visualization & Export" if lang_code == "en" else "å¯è¦–åŒ–ãƒ»å‡ºåŠ›"))
    st.markdown(
        "- **List** (sort/filter)\n- **Bubble Chart** (TF-IDF Ã— Freq Ã— Length)\n- **Word Cloud** / save PNG"
        if lang_code == "en" else
        "- **ãƒªã‚¹ãƒˆ**ï¼ˆä¸¦æ›¿/å“è©ãƒ•ã‚£ãƒ«ã‚¿ï¼‰\n- **ãƒãƒ–ãƒ«**ï¼ˆTF-IDFÃ—é »åº¦Ã—èªé•·ï¼‰\n- **ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰** / PNGä¿å­˜"
    )

st.divider()

# ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ
st.subheader("ğŸš€ " + ("Quick Start" if lang_code == "en" else "ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ"))
if lang_code == "ja":
    st.markdown(
        "1. å·¦ã® **ã€Œ2_åˆ†æãƒ„ãƒ¼ãƒ«ã€** ãƒšãƒ¼ã‚¸ã¸ç§»å‹•\n"
        "2. ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ› or `.txt` ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n"
        "3. **åˆ†æé–‹å§‹** ã‚’ã‚¯ãƒªãƒƒã‚¯\n"
        "4. ã‚¿ãƒ–ã§ **ãƒªã‚¹ãƒˆ / ãƒãƒ–ãƒ« / ã‚¯ãƒ©ã‚¦ãƒ‰** ã‚’ç¢ºèª\n"
        "5. å¿…è¦ãªã‚‰ **PNGä¿å­˜** ãƒœã‚¿ãƒ³ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n"
    )
else:
    st.markdown(
        "1. Go to **'2_Analysis Tool'** page from the sidebar\n"
        "2. Enter text or upload `.txt`\n"
        "3. Click **Start Analysis**\n"
        "4. View results in **List / Bubble / Cloud** tabs\n"
        "5. Save PNG if needed\n"
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ çµæœã®èª­ã¿æ–¹ï¼ˆã¾ã¨ã‚ï¼‰
with st.expander("ğŸ“ " + ("How to read the results (metrics)" if lang_code == "en" else "çµæœã®èª­ã¿æ–¹ï¼ˆæŒ‡æ¨™ã®æ„å‘³ï¼‰"), expanded=False):
    if lang_code == "ja":
        st.markdown(
            "**TF-IDF**: ãã®èªãŒ *æ–‡æ›¸ã«ç‰¹æœ‰* ã§ã‚ã‚‹åº¦åˆã„ï¼ˆé«˜ã„ã»ã©ç‰¹æœ‰ï¼‰ã€‚\n\n"
            "- **ãƒªã‚¹ãƒˆ**: ä¸Šã»ã©é‡è¦ï¼ˆé¸ã‚“ã ä¸¦æ›¿åŸºæº–ï¼‰ã€‚**é »åº¦**ï¼å‡ºç¾å›æ•°ã€**å“è©**ã¯spaCyã€‚\n"
            "- **ãƒãƒ–ãƒ«**: å³ï¼ˆTF-IDFâ†‘ï¼‰Ã— ä¸Šï¼ˆé »åº¦â†‘ï¼‰ãŒâ€œç‰¹æœ‰ã‹ã¤å¤šç”¨â€ã€‚ã‚µã‚¤ã‚ºï¼èªã®æ–‡å­—æ•°ã€è‰²ï¼å“è©ã€‚\n"
            "- **ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰**: å¤§ãã„èªã»ã©TF-IDFãŒé«˜ã„ã€‚**é…ç½®ã‚„è‰²ã¯è£…é£¾**ï¼ˆæ„å‘³ãªã—ï¼‰ã€‚\n"
            "\nè©³ç´°ã¯å„ãƒšãƒ¼ã‚¸æœ«å°¾ã®â€œã“ã®çµæœã®èª­ã¿æ–¹â€ã‚‚å‚ç…§ã—ã¦ãã ã•ã„ã€‚"
        )
    else:
        st.markdown(
            "**TF-IDF**: Degree to which a term is *specific to the document* (higher = more specific).\n\n"
            "- **List**: Higher rows are more important (by the selected sort key). **Frequency** = occurrences; **POS** from spaCy.\n"
            "- **Bubble**: Right (TF-IDFâ†‘) Ã— Up (Frequencyâ†‘) means â€œspecific and frequently used.â€ Size = term length (chars), color = POS.\n"
            "- **Word Cloud**: Larger words have higher TF-IDF. **Layout/colors are decorative** (no meaning).\n"
            "\nFor details, also see â€œHow to read these resultsâ€ at the bottom of each page."
        )


st.divider()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ¯”è¼ƒã‚³ãƒ¼ãƒ‘ã‚¹ä¸€è¦§ï¼ˆæŠ˜ã‚ŠãŸãŸã¿ï¼‰
st.subheader("ğŸ“š " + ("Reference Corpus (corpus/)" if lang_code == "en" else "æ¯”è¼ƒã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆcorpus/ï¼‰"))
with st.expander("See the breakdown of doc1â€“doc10" if lang_code == "en" else "doc1ã€œdoc10 ã®å†…è¨³ã‚’è¦‹ã‚‹"):

    st.markdown(
        "- **doc1**: BERT â€” https://arxiv.org/abs/1810.04805\n"
        "- **doc2**: Transformer â€” https://arxiv.org/abs/1706.03762\n"
        "- **doc3**: GPT â€” https://www.semanticscholar.org/paper/GPT-(OpenAI)-2018\n"
        "- **doc4**: ResNet â€” https://arxiv.org/abs/1512.03385\n"
        "- **doc5**: RoBERTa â€” https://arxiv.org/abs/1907.11692\n"
        "- **doc6**: GPTâ€‘3 â€” https://arxiv.org/abs/2005.14165\n"
        "- **doc7**: XAI â€” https://arxiv.org/abs/2006.11371\n"
        "- **doc8**: AI Planning & NLP â€” https://dl.acm.org/\n"
        "- **doc9**: T5 â€” https://arxiv.org/abs/1910.10683\n"
        "- **doc10**: Automatic Summarization â€” https://arxiv.org/abs/1704.04289"
    )

st.divider()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ç’°å¢ƒã¨ä½¿ã„æ–¹ï¼ˆ2ã‚«ãƒ©ãƒ ï¼‰
left, right = st.columns(2)
with left:
    st.subheader("ğŸ§© " + ("Development Environment" if lang_code == "en" else "é–‹ç™ºç’°å¢ƒ"))
    st.code(
        "- Python 3.x\n"
        "- Streamlit\n"
        "- spaCyï¼ˆen_core_web_smï¼‰\n"
        "- NLTK / scikit-learn\n"
        "- matplotlib / plotlyï¼ˆkaleidoï¼‰\n"
        "- wordcloud",
        language="markdown",
    )
with right:
    st.subheader("ğŸ”§ " + ("Setup" if lang_code == "en" else "ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"))
    st.markdown(
        "```bash\n"
        "pip install -r requirements.txt\n"
        "python -m spacy download en_core_web_sm\n"
        "streamlit run app.py\n"
        "```"
    )


st.caption("Â© Tsubasa Sato / Contact: s1290116@u-aizu.ac.jp")
