Building on the success of BERT, we present RoBERTa: a robustly optimized BERT pretraining approach. We show that carefully tuning hyperparameters—such as the learning rate, batch size, and training time—and removing the next sentence prediction objective significantly improves performance. We also increase training set size and adopt a dynamic masking pattern, training models on longer sequences with larger batch sizes. In practice, our models outperform existing systems across GLUE, RACE, and SQuAD benchmarks. This demonstrates the importance of experimental rigor and large-scale data when pretraining language models.
