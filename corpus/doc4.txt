Deep convolutional neural networks have higher representational capacity but are harder to train as they go deeper — often suffering from degradation. To address this, we propose a Residual Learning framework that eases the training of networks that are substantially deeper than previously used. Instead of hoping each few layers directly fit a desired underlying mapping, we explicitly let these layers fit a residual mapping. We stack identity shortcut connections that skip one or more layers. This architecture allows identity mappings to be learned easily while still increasing capacity. In our experiments, deep residual networks of up to 152 layers were trained using stochastic gradient descent and batch normalization. These networks achieved impressive results on ImageNet, significantly reducing top‑5 error compared to prior architectures like VGG.
