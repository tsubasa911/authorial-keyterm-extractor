Transfer learning, in which a model is first pre-trained on a large dataset and then fine-tuned on a more specific task, has become standard practice in NLP. In this work, we present the Text-To-Text Transfer Transformer (T5), a unified framework that converts every language problem into a text-to-text format. By training on a large multi-task mixture of datasets, T5 can learn to perform summarization, question answering, translation, and classification using the same model and objective. We explore the effects of different data mixtures, model sizes, and training objectives. Our best-performing model achieves state-of-the-art results across a wide variety of benchmarks, demonstrating the versatility and scalability of the text-to-text approach.
