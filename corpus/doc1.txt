We introduce BERT, a method for pre-training language representations using bidirectional transformers. BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context.

Unlike previous models, BERT enables fine-tuning with minimal architecture changes, achieving state-of-the-art performance on a wide range of NLP tasks such as question answering and natural language inference.

BERT uses two pre-training objectives: masked language modeling and next sentence prediction. The masked language model randomly masks tokens and learns to predict them, enabling deep bidirectional understanding. The next sentence prediction task helps in learning relationships between sentences.

Experiments show that BERT significantly outperforms previous methods across eleven NLP tasks.
